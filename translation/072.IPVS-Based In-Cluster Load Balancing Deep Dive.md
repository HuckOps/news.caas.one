lgtm

作者：杜军（华为），谢海滨（华为），魏亮（华为）

# 介绍
根据Kubernetes 1.11版本的博客文章，我们声明基于IPVS的集群服务内的负载均衡已逐步升级为通用。 在此博客中，我们将带您深入了解其特性。

# 什么是IPVS？
IPVS（IP虚拟服务器）构建在Netfilter之上，并作为Linux内核的一部分实现传输层负载平衡。

IPVS被集成到LVS（Linux虚拟服务器）中，在此服务器上运行，并充当真实服务器集群之前的负载平衡器。 IPVS可以将基于TCP和UDP的服务的请求定向到真实服务器，并使真实服务器的服务显示为在一个IP地址上的虚拟服务。 因此，IPVS自然支持Kubernetes服务。

# 为什么Kubernetes要使用IPVS？
随着Kubernetes的使用不断增长，其资源的可伸缩性变得越来越重要。特别是，服务的可伸缩性对于有着巨大业务量的开发人员/公司采用Kubernetes是至关重要的。

服务路由的构建模块Kube-proxy依靠多次修改后的iptables来实现其主要支持的服务类型，例如ClusterIP和NodePort。但是，iptables很难扩展到成千上万的服务，因为它纯粹是为防火墙目的而设计的，并且基于内核规则列表。

尽管Kubernetes在v1.6版本中已经支持5000个节点，但是基于iptables的kube-proxy实际上是将集群扩展到5000个节点的瓶颈。例如，在5000个节点的集群中使用NodePort Service，如果我们有2000个服务，而每个服务有10个Pod，则这将导致在每个工作节点上至少20000条iptable记录，这会使内核变得非常繁忙。

另一方面，使用基于IPVS的集群服务内的负载均衡在这种情况下会很有帮助。 IPVS是专门为负载平衡而设计的，并使用更有效的数据结构（哈希表），允许在后台进行几乎无限的扩展。

# 基于IPVS的Kube-proxy
## 参数变化
**参数：**`–proxy-mode`

除了现有的用户空间和iptables模式外，还通过--proxy-mode = ipvs配置IPVS模式。它隐式使用IPVS NAT模式进行服务端口映射。

**参数：**`–ipvs-scheduler`

添加了新的kube-proxy参数来指定IPVS负载平衡算法，该参数为--ipvs-scheduler。如果未配置，则轮询（rr）为默认值。

 - rr：轮询 
 - lc：最少连接数量 
 - dh：目标地址哈希 
 - sh：源地址哈希 
 - sed：最短的预期延迟 
 - nq：永不排队

将来，我们可以实现通过服务指定调度策略（可能通过annotation），并覆盖默认值。

**参数：**`--cleanup-ipvs`

与`--cleanup-iptables`参数相似，如果为true，则清除在IPVS模式下创建的IPVS配置和IPTables规则。

**参数：**`--ipvs-sync-period`

刷新IPVS规则的最大间隔（例如“ 5s”，“ 1m”）。必须大于0。

**参数：**`--ipvs-min-sync-period`

刷新IPVS规则的最小间隔（例如“ 5s”，“ 1m”）。必须大于0。

**参数：**`--ipvs-exclude-cidrs`

指定一个使用逗号分隔的CIDR列表，这个列表规定了IPVS刷新时不会被清理的规则。因为IPVS代理无法将kube-proxy创建的IPVS规则与用户原始IPVS规则区分开。如果您在环境中使用具有自己的IPVS规则的IPVS代理，则应指定此参数，否则将清除原始规则。

## 设计注意事项
**IPVS服务网络拓扑**
创建ClusterIP类型服务时，IPVS模式的kube-proxy将执行以下三件事：

- 确保节点中存在一个虚拟网卡，默认为kube-ipvs0
- 将服务IP地址绑定到虚拟网卡
- 分别为每个服务IP地址创建IPVS虚拟服务器

下面是一个示例：

```bash
# kubectl describe svc nginx-service
Name:			nginx-service
...
Type:			ClusterIP
IP:			    10.102.128.4
Port:			http	3080/TCP
Endpoints:		10.244.0.235:8080,10.244.1.237:8080
Session Affinity:	None

# ip addr
...
73: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
    inet 10.102.128.4/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn     
TCP  10.102.128.4:3080 rr
  -> 10.244.0.235:8080            Masq    1      0          0         
  -> 10.244.1.237:8080            Masq    1      0          0   
```
请注意，Kubernetes服务和IPVS虚拟服务器之间的关系是1：N。 例如，考虑具有多个IP地址的Kubernetes服务。 ExternalIP类型的服务，就有clusterIP和ExternalIP两个地址。 然后，IPVS代理将创建2个IPVS虚拟服务器，一个用于clusterIP，另一个用于ExternalI。而Kubernetes Endpoint（每个IP +端口对）和IPVS虚拟服务器之间的关系是1：1。

删除Kubernetes服务将触发相应IPVS虚拟服务器，IPVS真实服务器及其绑定到虚拟网卡的IP地址的删除。

**端口映射**
IPVS中有三种代理模式：NAT（masq），IPIP和DR。 仅NAT模式支持端口映射。 Kube-proxy利用NAT模式进行端口映射。 以下示例显示IPVS将服务端口3080映射到Pod端口8080。

```bash
TCP  10.102.128.4:3080 rr
  -> 10.244.0.235:8080            Masq    1      0          0         
  -> 10.244.1.237:8080            Masq    1      0       
```
**会话保持**
IPVS支持客户端IP会话保持（持久连接）。 当服务指定会话保持时，IPVS代理将在IPVS虚拟服务器中设置超时时间（默认为180min = 10800s）。 例如：

```bash
# kubectl describe svc nginx-service
Name:			nginx-service
...
IP:			    10.102.128.4
Port:			http	3080/TCP
Session Affinity:	ClientIP

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.102.128.4:3080 rr persistent 10800
```
**IPVS Proxier中的iptables和ipset**
IPVS用于负载均衡，无法实现kube-proxy中的其他功能，例如 数据包过滤， hairpin-masquerade tricks，源地址转换等

IPVS代理在上述情况下利用iptables实现。 具体来说，在以下4种情况下，ipvs proxier将依赖iptables：
- kube-proxy包含参数–masquerade-all = true
- 在kube-proxy启动指定CIDR参数
- 支持负载均衡器类型服务
- 支持NodePort类型服务

但是，我们不想创建太多iptables规则。 因此，为了减少iptables规则，我们采用ipset。 下表是IPVS代理维护的ipset列表：

| set name                       | members                                                      | usage                                                        |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| KUBE-CLUSTER-IP                | 所有服务IP +端口                                             | 指定参数masquerade-all = true或clusterCIDR则作masquerade功能 |
| KUBE-LOOP-BACK                 | 所有服务IP +端口+ IP                                         | 作masquerade功能，应对hairpin问题                            |
| KUBE-EXTERNAL-IP               | 服务 External IP +端口                                       | 为访问external IP的数据包作masquerade                        |
| KUBE-LOAD-BALANCER             | 负载均衡器 ingress IP +端口                                  | 为访问负载均衡服务的数据包作masquerade                       |
| KUBE-LOAD-BALANCER-LOCAL       | 使用参数externalTrafficPolicy = local的负载均衡器 ingress IP +端口 | 使用参数externalTrafficPolicy = local接受访问负载均衡服务的数据包 |
| KUBE-LOAD-BALANCER-FW          | 使用参数loadBalancerSourceRanges的负载均衡器 ingress IP +端口 | 指定参数loadBalancerSourceRanges丢弃访问负载均衡服务的数据包 |
| KUBE-LOAD-BALANCER-SOURCE-CIDR | 负载均衡器 ingress IP +端口+source CIDR                      | 接受指定参数loadBalancerSourceRanges的负载均衡器类型服务的数据包 |
| KUBE-NODE-PORT-TCP             | NodePort类型服务TCP端口                                      | 为访问NodePort(TCP)的数据包作masquerade                      |
| KUBE-NODE-PORT-LOCAL-TCP       | NodePort类型服务TCP端口，使用参数externalTrafficPolicy = local | 使用参数externalTrafficPolicy = local接受访问NodePort服务的数据包 |
| KUBE-NODE-PORT-UDP             | NodePort类型服务UDP端口                                      | 为访问NodePort(UDP)的数据包作masquerade                      |
| KUBE-NODE-PORT-LOCAL-UDP       | 使用参数externalTrafficPolicy = local的NodePort类型服务UDP端口 | 使用参数externalTrafficPolicy = local接受访问NodePort Service的数据包 |

通常，对于IPVS代理，无论我们拥有多少pod/service，iptables规则的数量都是固定的。

## 在IPVS模式下运行kube-proxy
目前，本地化脚本，GCE脚本以及kubeadm都支持通过导出环境变量（KUBE_PROXY_MODE = ipvs）或指定参数（`--proxy-mode = ipvs`）切换IPVS代理模式。 在运行IPVS代理之前，请确保已安装IPVS所需的内核模块。

```bash
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
```
最后，对于Kubernetes v1.10，特性开关SupportIPVSProxyMode默认设置为true。 对于Kubernetes v1.11，特性开关已被完全移除。 然而，您需要在v1.10之前为Kubernetes启用参数--feature-gates = SupportIPVSProxyMode = true来使用IPVS模式。
